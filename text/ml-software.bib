
@inproceedings{krizhevsky_imagenet_2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}.},
  volume = {1},
  url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
  timestamp = {2014-04-09T16:31:42Z},
  booktitle = {{{NIPS}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  urldate = {2014-04-09},
  date = {2012},
  pages = {4},
  file = {Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf:/Users/jschmolling/Dropbox/ebooks/Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf:application/pdf}
}

@article{graves2013generating,
  title = {Generating sequences with recurrent neural networks},
  timestamp = {2015-12-03T23:02:34Z},
  journaltitle = {arXiv preprint arXiv:1308.0850},
  author = {Graves, Alex},
  date = {2013},
  file = {Graves_2013_Generating sequences with recurrent neural networks.pdf:/Users/jschmolling/Dropbox/ebooks/Graves_2013_Generating sequences with recurrent neural networks.pdf:application/pdf}
}

@article{mikolov_efficient_2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  url = {http://arxiv.org/abs/1301.3781},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  timestamp = {2016-04-26T22:13:34Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1301.3781},
  primaryClass = {cs},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  urldate = {2016-04-26},
  date = {2013-01-16},
  keywords = {Computer Science - Computation and Language},
  file = {Mikolov_et_al_2013_Efficient_Estimation_of_Word_Representations_in_Vector_Space.pdf:/Users/jschmolling/Dropbox/ebooks/Mikolov_et_al_2013_Efficient_Estimation_of_Word_Representations_in_Vector_Space.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/DB9AGMB2/1301.html:}
}

@article{nielsen_neural_2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  url = {http://neuralnetworksanddeeplearning.com},
  timestamp = {2016-04-13T14:35:16Z},
  author = {Nielsen, Michael A.},
  urldate = {2016-04-13},
  date = {2015},
  file = {Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/8AHVRUNR/neuralnetworksanddeeplearning.com.html:}
}

@article{radford_unsupervised_2015,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  url = {http://arxiv.org/abs/1511.06434},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  timestamp = {2016-05-04T12:33:41Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.06434},
  primaryClass = {cs},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  urldate = {2016-05-04},
  date = {2015-11-19},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  file = {Radford_et_al_2015_Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative.pdf:/Users/jschmolling/Dropbox/ebooks/Radford_et_al_2015_Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/UXU8UF8T/1511.html:}
}

@article{gregor_draw:_2015,
  title = {{{DRAW}}: {{A Recurrent Neural Network For Image Generation}}},
  url = {http://arxiv.org/abs/1502.04623},
  shorttitle = {{{DRAW}}},
  abstract = {This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.},
  timestamp = {2016-04-19T12:37:32Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.04623},
  primaryClass = {cs},
  author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  urldate = {2016-04-19},
  date = {2015-02-16},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Gregor_et_al_2015_DRAW.pdf:/Users/jschmolling/Dropbox/ebooks/Gregor_et_al_2015_DRAW.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/MRS4RMHA/1502.html:}
}

@article{lazaridou_unveiling_2015,
  title = {Unveiling the {{Dreams}} of {{Word Embeddings}}: {{Towards Language-Driven Image Generation}}},
  url = {http://arxiv.org/abs/1506.03500},
  shorttitle = {Unveiling the {{Dreams}} of {{Word Embeddings}}},
  abstract = {We introduce language-driven image generation, the task of generating an image visualizing the semantic contents of a word embedding, e.g., given the word embedding of grasshopper, we generate a natural image of a grasshopper. We implement a simple method based on two mapping functions. The first takes as input a word embedding (as produced, e.g., by the word2vec toolkit) and maps it onto a high-level visual space (e.g., the space defined by one of the top layers of a Convolutional Neural Network). The second function maps this abstract visual representation to pixel space, in order to generate the target image. Several user studies suggest that the current system produces images that capture general visual properties of the concepts encoded in the word embedding, such as color or typical environment, and are sufficient to discriminate between general categories of objects.},
  timestamp = {2016-04-19T18:56:32Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.03500},
  primaryClass = {cs},
  author = {Lazaridou, Angeliki and Nguyen, Dat Tien and Bernardi, Raffaella and Baroni, Marco},
  urldate = {2016-04-19},
  date = {2015-06-10},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {Lazaridou_et_al_2015_Unveiling_the_Dreams_of_Word_Embeddings.pdf:/Users/jschmolling/Dropbox/ebooks/Lazaridou_et_al_2015_Unveiling_the_Dreams_of_Word_Embeddings.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/EIS5GVND/1506.html:}
}

@article{mahendran_understanding_2014,
  title = {Understanding {{Deep Image Representations}} by {{Inverting Them}}},
  url = {http://arxiv.org/abs/1412.0035},
  abstract = {Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
  timestamp = {2016-04-26T22:16:19Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.0035},
  primaryClass = {cs},
  author = {Mahendran, Aravindh and Vedaldi, Andrea},
  urldate = {2016-04-26},
  date = {2014-11-26},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Mahendran_Vedaldi_2014_Understanding_Deep_Image_Representations_by_Inverting_Them.pdf:/Users/jschmolling/Dropbox/ebooks/Mahendran_Vedaldi_2014_Understanding_Deep_Image_Representations_by_Inverting_Them.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/SFHVHRBG/1412.html:}
}

@online{_classification_????,
  title = {Classification datasets results},
  url = {https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html},
  timestamp = {2016-05-11T20:46:50Z},
  urldate = {2016-05-11},
  file = {Classification_datasets_results.mhtml:/Users/jschmolling/Dropbox/ebooks/Classification_datasets_results.mhtml:}
}

@article{goodfellow_generative_2014,
  title = {Generative {{Adversarial Networks}}},
  url = {http://arxiv.org/abs/1406.2661},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  timestamp = {2016-05-04T12:40:15Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2661},
  primaryClass = {cs, stat},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  urldate = {2016-05-04},
  date = {2014-06-10},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {Goodfellow_et_al_2014_Generative_Adversarial_Networks.pdf:/Users/jschmolling/Dropbox/ebooks/Goodfellow_et_al_2014_Generative_Adversarial_Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/VEKWA43N/1406.html:}
}

@online{_building_????,
  title = {Building {{Autoencoders}} in {{Keras}}},
  url = {http://blog.keras.io/building-autoencoders-in-keras.html},
  timestamp = {2016-05-22T15:45:31Z},
  urldate = {2016-05-22},
  file = {Building Autoencoders in Keras:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/84GQDCB9/building-autoencoders-in-keras.html:}
}

@article{lecun_deep_2015,
  title = {Deep learning},
  volume = {521},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  timestamp = {2016-06-07T22:13:48Z},
  number = {7553},
  journaltitle = {Nature},
  shortjournal = {Nature},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015-05-28},
  pages = {436--444},
  keywords = {Algorithms,Artificial Intelligence,Computers,Language,Neural Networks (Computer)},
  file = {lecun2015.pdf:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/JFUBJ26X/lecun2015.pdf:application/pdf},
  eprinttype = {pmid},
  eprint = {26017442}
}

@online{_caffe_????,
  title = {Caffe | {{Deep Learning Framework}}},
  url = {http://caffe.berkeleyvision.org/},
  timestamp = {2015-11-23T16:43:29Z},
  urldate = {2015-11-23}
}

@online{_fchollet/keras_????,
  title = {fchollet/keras},
  url = {https://github.com/fchollet/keras},
  timestamp = {2015-11-23T16:43:43Z},
  urldate = {2015-11-23}
}

@article{the_theano_development_team_theano:_2016,
  title = {Theano: {{A Python}} framework for fast computation of mathematical expressions},
  url = {http://arxiv.org/abs/1605.02688},
  shorttitle = {Theano},
  abstract = {Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.},
  timestamp = {2016-05-25T22:29:43Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.02688},
  primaryClass = {cs},
  author = {The Theano Development Team and Al-Rfou, Rami and Alain, Guillaume and Almahairi, Amjad and Angermueller, Christof and Bahdanau, Dzmitry and Ballas, Nicolas and Bastien, Frédéric and Bayer, Justin and Belikov, Anatoly and Belopolsky, Alexander and Bengio, Yoshua and Bergeron, Arnaud and Bergstra, James and Bisson, Valentin and Snyder, Josh Bleecher and Bouchard, Nicolas and Boulanger-Lewandowski, Nicolas and Bouthillier, Xavier and de Brébisson, Alexandre and Breuleux, Olivier and Carrier, Pierre-Luc and Cho, Kyunghyun and Chorowski, Jan and Christiano, Paul and Cooijmans, Tim and Côté, Marc-Alexandre and Côté, Myriam and Courville, Aaron and Dauphin, Yann N. and Delalleau, Olivier and Demouth, Julien and Desjardins, Guillaume and Dieleman, Sander and Dinh, Laurent and Ducoffe, Mélanie and Dumoulin, Vincent and Kahou, Samira Ebrahimi and Erhan, Dumitru and Fan, Ziye and Firat, Orhan and Germain, Mathieu and Glorot, Xavier and Goodfellow, Ian and Graham, Matt and Gulcehre, Caglar and Hamel, Philippe and Harlouchet, Iban and Heng, Jean-Philippe and Hidasi, Balázs and Honari, Sina and Jain, Arjun and Jean, Sébastien and Jia, Kai and Korobov, Mikhail and Kulkarni, Vivek and Lamb, Alex and Lamblin, Pascal and Larsen, Eric and Laurent, César and Lee, Sean and Lefrancois, Simon and Lemieux, Simon and Léonard, Nicholas and Lin, Zhouhan and Livezey, Jesse A. and Lorenz, Cory and Lowin, Jeremiah and Ma, Qianli and Manzagol, Pierre-Antoine and Mastropietro, Olivier and McGibbon, Robert T. and Memisevic, Roland and van Merriënboer, Bart and Michalski, Vincent and Mirza, Mehdi and Orlandi, Alberto and Pal, Christopher and Pascanu, Razvan and Pezeshki, Mohammad and Raffel, Colin and Renshaw, Daniel and Rocklin, Matthew and Romero, Adriana and Roth, Markus and Sadowski, Peter and Salvatier, John and Savard, François and Schlüter, Jan and Schulman, John and Schwartz, Gabriel and Serban, Iulian Vlad and Serdyuk, Dmitriy and Shabanian, Samira and Simon, Étienne and Spieckermann, Sigurd and Subramanyam, S. Ramana and Sygnowski, Jakub and Tanguay, Jérémie and van Tulder, Gijs and Turian, Joseph and Urban, Sebastian and Vincent, Pascal and Visin, Francesco and de Vries, Harm and Warde-Farley, David and Webb, Dustin J. and Willson, Matthew and Xu, Kelvin and Xue, Lijun and Yao, Li and Zhang, Saizheng and Zhang, Ying},
  urldate = {2016-05-25},
  date = {2016-05-09},
  keywords = {Computer Science - Learning,Computer Science - Mathematical Software,Computer Science - Symbolic Computation},
  options = {useprefix=true},
  file = {The_Theano_Development_Team_et_al_2016_Theano.pdf:/Users/jschmolling/Dropbox/ebooks/The_Theano_Development_Team_et_al_2016_Theano.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/3N85IQMB/1605.html:}
}

@article{tensorflow2015-whitepaper,
  title = {{{TensorFlow}}: {{Large-Scale Machine Learning}} on {{Heterogeneous Systems}}},
  url = {http://tensorflow.org/},
  timestamp = {2016-06-07T23:54:03Z},
  author = {Abadi, Mart́ın and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and S.~Corrado, Greg and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  date = {2015},
  note = {Software available from tensorflow.org},
  file = {Abadi_et_al_2015_TensorFlow.pdf:/Users/jschmolling/Dropbox/ebooks/Abadi_et_al_2015_TensorFlow.pdf:application/pdf}
}

@online{_mxnet_????,
  title = {{{MXNet Documents}}},
  url = {https://mxnet.readthedocs.io/en/latest/},
  timestamp = {2016-06-12T12:11:31Z},
  urldate = {2016-06-12},
  file = {MXNet Documents:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/SCIRU2JJ/latest.html:}
}

@inreference{_autoencoder_2016,
  title = {Autoencoder},
  rights = {Creative Commons Attribution-ShareAlike License},
  url = {https://en.wikipedia.org/w/index.php?title=Autoencoder&oldid=720751900},
  abstract = {An autoencoder, autoassociator or Diabolo network is an artificial neural network used for learning efficient codings. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. Recently, the autoencoder concept has become more widely used for learning generative models of data.},
  timestamp = {2016-05-18T13:23:07Z},
  langid = {german},
  booktitle = {Wikipedia},
  urldate = {2016-05-18},
  date = {2016-05-17},
  note = {Page Version ID: 720751900},
  file = {Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/UWP9DSJC/index.html:}
}

@inproceedings{graves_connectionist_2006,
  title = {Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
  url = {http://dl.acm.org/citation.cfm?id=1143891},
  shorttitle = {Connectionist temporal classification},
  timestamp = {2016-06-07T23:52:26Z},
  booktitle = {Proceedings of the 23rd international conference on {{Machine}} learning},
  publisher = {{ACM}},
  author = {Graves, Alex and Fernández, Santiago and Gomez, Faustino and Schmidhuber, Jürgen},
  urldate = {2016-06-07},
  date = {2006},
  pages = {369--376},
  file = {Graves_et_al_2006_Connectionist_temporal_classification.pdf:/Users/jschmolling/Dropbox/ebooks/Graves_et_al_2006_Connectionist_temporal_classification.pdf:application/pdf;Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/DJPI35P2/citation.html:}
}

@online{_saiprashanths/dl-setup_????,
  title = {saiprashanths/dl-setup},
  url = {https://github.com/saiprashanths/dl-setup},
  abstract = {dl-setup - Instructions for setting up the software on your deep learning machine},
  timestamp = {2016-06-08T00:00:41Z},
  journaltitle = {GitHub},
  urldate = {2016-06-08},
  file = {Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/USM2N7P6/dl-setup.html:}
}

@online{_cs231n_2016,
  title = {{{CS231n Convolutional Neural Networks}} for {{Visual Recognition}}},
  url = {https://cs231n.github.io/},
  timestamp = {2016-06-08T00:41:42Z},
  urldate = {2016-06-08},
  date = {2016},
  file = {CS231n Convolutional Neural Networks for Visual Recognition:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/5K62A6SE/cs231n.github.io.html:}
}

@comment{jabref-meta: groupsversion:3;}
@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:frameworks\;0\;_caffe_????\;_fchollet/keras_????\;the_
theano_development_team_theano:_2016\;tensorflow2015-whitepaper\;_mxne
t_????\;;
1 ExplicitGroup:knn-types\;0\;_autoencoder_2016\;graves_connectionist_
2006\;;
1 ExplicitGroup:problems\;0\;_autoencoder_2016\;_building_????\;;
1 ExplicitGroup:setup\;0\;_saiprashanths/dl-setup_????\;;
1 ExplicitGroup:theory\;0\;lecun_deep_2015\;_cs231n_2016\;;
}

